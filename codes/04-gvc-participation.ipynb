{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GVC participation rate\n",
    "\n",
    "This notebook computes the trade-based and production-based GVC participation rates at the country-level and the 5-, 15-, and 35-sector levels. It uses the trade accounting framework of Borin and Mancini (2019). Results are saved in `data/`.\n",
    "\n",
    "The trade-based GVC participation rate is defined as\n",
    "```\n",
    "GVCP_trade_f = (REX1 + REX2 + REX3 + REF1 + REF2) / Exports\n",
    "GVCP_trade_f = (FVA + PDC1 + PDC2) / Exports\n",
    "GVCP_trade = GVCP_trade_f + GVCP_trade_b\n",
    "```\n",
    "where sectors are broken down by export sectors.\n",
    "\n",
    "The production-based GVC participation rate, meanwhile, is defined as\n",
    "```\n",
    "GVCP_prod = (DAVAX2 + REX1 + REX2 + REX3 + REF1 + REF2) / va\n",
    "```\n",
    "where sectors are broken down by origin sectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import duckdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ta, summary, output = 'ta.parquet', 'summary.parquet', 'gvcp.parquet'\n",
    "# ta, summary, output = 'ta62.parquet', 'summary62.parquet', 'gvcp62.parquet'\n",
    "# ta, summary, output = 'ta62-const.parquet', 'summary62-const.parquet', 'gvcp62-const.parquet'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and process data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Breakdown by export sectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_es = duckdb.sql(\n",
    "    f\"\"\"\n",
    "    (SELECT t, s, 0 AS agg, 0 AS i, \n",
    "        sum(Exports) AS Exports, \n",
    "        sum(DAVAX1) AS DAVAX1,\n",
    "        sum(DAVAX2) AS DAVAX2,\n",
    "        sum(REX1) AS REX1,\n",
    "        sum(REX2) AS REX2,\n",
    "        sum(REX3) AS REX3,\n",
    "        sum(REF1) AS REF1,\n",
    "        sum(REF2) AS REF2,\n",
    "        sum(FVA) AS FVA,\n",
    "        sum(PDC1) AS PDC1,\n",
    "        sum(PDC2) AS PDC2\n",
    "    FROM read_parquet('../data/{ta}') WHERE breakdown='none'\n",
    "    GROUP BY t, s\n",
    "    ORDER BY t, s)\n",
    "\n",
    "    UNION ALL\n",
    "\n",
    "    (SELECT t, s, 5 AS agg, i5 AS i, \n",
    "        sum(Exports) AS Exports, \n",
    "        sum(DAVAX1) AS DAVAX1,\n",
    "        sum(DAVAX2) AS DAVAX2,\n",
    "        sum(REX1) AS REX1,\n",
    "        sum(REX2) AS REX2,\n",
    "        sum(REX3) AS REX3,\n",
    "        sum(REF1) AS REF1,\n",
    "        sum(REF2) AS REF2,\n",
    "        sum(FVA) AS FVA,\n",
    "        sum(PDC1) AS PDC1,\n",
    "        sum(PDC2) AS PDC2\n",
    "    FROM read_parquet('../data/{ta}') WHERE breakdown='es'\n",
    "    GROUP BY t, s, i5\n",
    "    ORDER BY t, s, i5)\n",
    "\n",
    "    UNION ALL\n",
    "    \n",
    "    (SELECT t, s, 15 AS agg, i15 AS i, \n",
    "        sum(Exports) AS Exports, \n",
    "        sum(DAVAX1) AS DAVAX1,\n",
    "        sum(DAVAX2) AS DAVAX2,\n",
    "        sum(REX1) AS REX1,\n",
    "        sum(REX2) AS REX2,\n",
    "        sum(REX3) AS REX3,\n",
    "        sum(REF1) AS REF1,\n",
    "        sum(REF2) AS REF2,\n",
    "        sum(FVA) AS FVA,\n",
    "        sum(PDC1) AS PDC1,\n",
    "        sum(PDC2) AS PDC2\n",
    "    FROM read_parquet('../data/{ta}') WHERE breakdown='es'\n",
    "    GROUP BY t, s, i15\n",
    "    ORDER BY t, s, i15)\n",
    "    \n",
    "    UNION ALL\n",
    "    \n",
    "    (SELECT t, s, 35 AS agg, i, \n",
    "        sum(Exports) AS Exports, \n",
    "        sum(DAVAX1) AS DAVAX1,\n",
    "        sum(DAVAX2) AS DAVAX2,\n",
    "        sum(REX1) AS REX1,\n",
    "        sum(REX2) AS REX2,\n",
    "        sum(REX3) AS REX3,\n",
    "        sum(REF1) AS REF1,\n",
    "        sum(REF2) AS REF2,\n",
    "        sum(FVA) AS FVA,\n",
    "        sum(PDC1) AS PDC1,\n",
    "        sum(PDC2) AS PDC2\n",
    "    FROM read_parquet('../data/{ta}') WHERE breakdown='es'\n",
    "    GROUP BY t, s, i\n",
    "    ORDER BY t, s, i)\n",
    "    \"\"\"\n",
    ").df()\n",
    "\n",
    "df_es['GVC_trade_f'] = df_es['REX1'] + df_es['REX2'] + df_es['REX3'] + df_es['REF1'] + df_es['REF2']\n",
    "df_es['GVC_trade_b'] = df_es['FVA'] + df_es['PDC1'] + df_es['PDC2']\n",
    "df_es['GVC_trade'] = df_es['GVC_trade_f'] + df_es['GVC_trade_b']\n",
    "df_es['t'] = df_es['t'].astype(int)\n",
    "df_es = df_es[['t', 's', 'agg', 'i', 'Exports', 'GVC_trade_f', 'GVC_trade_b', 'GVC_trade']]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Breakdown by origin sectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_os = duckdb.sql(\n",
    "    f\"\"\"\n",
    "    (SELECT t, s, 0 AS agg, 0 AS i, \n",
    "        sum(Exports) AS Exports, \n",
    "        sum(DAVAX1) AS DAVAX1,\n",
    "        sum(DAVAX2) AS DAVAX2,\n",
    "        sum(REX1) AS REX1,\n",
    "        sum(REX2) AS REX2,\n",
    "        sum(REX3) AS REX3,\n",
    "        sum(REF1) AS REF1,\n",
    "        sum(REF2) AS REF2,\n",
    "        sum(FVA) AS FVA,\n",
    "        sum(PDC1) AS PDC1,\n",
    "        sum(PDC2) AS PDC2\n",
    "    FROM read_parquet('../data/{ta}') WHERE breakdown='none'\n",
    "    GROUP BY t, s\n",
    "    ORDER BY t, s)\n",
    "\n",
    "    UNION ALL\n",
    "\n",
    "    (SELECT t, s, 5 AS agg, i5 AS i, \n",
    "        sum(Exports) AS Exports, \n",
    "        sum(DAVAX1) AS DAVAX1,\n",
    "        sum(DAVAX2) AS DAVAX2,\n",
    "        sum(REX1) AS REX1,\n",
    "        sum(REX2) AS REX2,\n",
    "        sum(REX3) AS REX3,\n",
    "        sum(REF1) AS REF1,\n",
    "        sum(REF2) AS REF2,\n",
    "        sum(FVA) AS FVA,\n",
    "        sum(PDC1) AS PDC1,\n",
    "        sum(PDC2) AS PDC2\n",
    "    FROM read_parquet('../data/{ta}') WHERE breakdown='os'\n",
    "    GROUP BY t, s, i5\n",
    "    ORDER BY t, s, i5)\n",
    "\n",
    "    UNION ALL\n",
    "    \n",
    "    (SELECT t, s, 15 AS agg, i15 AS i, \n",
    "        sum(Exports) AS Exports, \n",
    "        sum(DAVAX1) AS DAVAX1,\n",
    "        sum(DAVAX2) AS DAVAX2,\n",
    "        sum(REX1) AS REX1,\n",
    "        sum(REX2) AS REX2,\n",
    "        sum(REX3) AS REX3,\n",
    "        sum(REF1) AS REF1,\n",
    "        sum(REF2) AS REF2,\n",
    "        sum(FVA) AS FVA,\n",
    "        sum(PDC1) AS PDC1,\n",
    "        sum(PDC2) AS PDC2\n",
    "    FROM read_parquet('../data/{ta}') WHERE breakdown='os'\n",
    "    GROUP BY t, s, i15\n",
    "    ORDER BY t, s, i15)\n",
    "    \n",
    "    UNION ALL\n",
    "    \n",
    "    (SELECT t, s, 35 AS agg, i, \n",
    "        sum(Exports) AS Exports, \n",
    "        sum(DAVAX1) AS DAVAX1,\n",
    "        sum(DAVAX2) AS DAVAX2,\n",
    "        sum(REX1) AS REX1,\n",
    "        sum(REX2) AS REX2,\n",
    "        sum(REX3) AS REX3,\n",
    "        sum(REF1) AS REF1,\n",
    "        sum(REF2) AS REF2,\n",
    "        sum(FVA) AS FVA,\n",
    "        sum(PDC1) AS PDC1,\n",
    "        sum(PDC2) AS PDC2\n",
    "    FROM read_parquet('../data/{ta}') WHERE breakdown='os'\n",
    "    GROUP BY t, s, i\n",
    "    ORDER BY t, s, i)\n",
    "    \"\"\"\n",
    ").df()\n",
    "\n",
    "df_os['GVC_prod'] = df_os['DAVAX2'] + df_os['REX1'] + df_os['REX2'] + df_os['REX3'] + df_os['REF1'] + df_os['REF2']\n",
    "df_os['t'] = df_os['t'].astype(int)\n",
    "df_os = df_os[['t', 's', 'agg', 'i', 'GVC_prod']]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "va = duckdb.sql(\n",
    "    f\"\"\"\n",
    "    (SELECT t, s, 0 AS agg, 0 AS i, sum(va) AS va, \n",
    "    FROM read_parquet('../data/{summary}')\n",
    "    GROUP BY t, s\n",
    "    ORDER BY t, s)\n",
    "\n",
    "    UNION ALL\n",
    "\n",
    "    (SELECT t, s, 5 AS agg, i5 AS i, sum(va) AS va, \n",
    "    FROM read_parquet('../data/{summary}')\n",
    "    GROUP BY t, s, i5\n",
    "    ORDER BY t, s, i5)\n",
    "\n",
    "    UNION ALL\n",
    "\n",
    "    (SELECT t, s, 15 AS agg, i15 AS i, sum(va) AS va, \n",
    "    FROM read_parquet('../data/{summary}')\n",
    "    GROUP BY t, s, i15\n",
    "    ORDER BY t, s, i15)\n",
    "\n",
    "    UNION ALL\n",
    "\n",
    "    (SELECT t, s, 35 AS agg, i, sum(va) AS va, \n",
    "    FROM read_parquet('../data/{summary}')\n",
    "    GROUP BY t, s, i\n",
    "    ORDER BY t, s, i)\n",
    "    \"\"\"\n",
    ").df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "You are trying to merge on int64 and object columns. If you wish to proceed you should use pd.concat",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df_os \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mmerge(df_os, va)\n",
      "File \u001b[0;32m~/Envs/adb/lib/python3.11/site-packages/pandas/core/reshape/merge.py:148\u001b[0m, in \u001b[0;36mmerge\u001b[0;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[39m@Substitution\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mleft : DataFrame or named Series\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    132\u001b[0m \u001b[39m@Appender\u001b[39m(_merge_doc, indents\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m    133\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmerge\u001b[39m(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    146\u001b[0m     validate: \u001b[39mstr\u001b[39m \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    147\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame:\n\u001b[0;32m--> 148\u001b[0m     op \u001b[39m=\u001b[39m _MergeOperation(\n\u001b[1;32m    149\u001b[0m         left,\n\u001b[1;32m    150\u001b[0m         right,\n\u001b[1;32m    151\u001b[0m         how\u001b[39m=\u001b[39;49mhow,\n\u001b[1;32m    152\u001b[0m         on\u001b[39m=\u001b[39;49mon,\n\u001b[1;32m    153\u001b[0m         left_on\u001b[39m=\u001b[39;49mleft_on,\n\u001b[1;32m    154\u001b[0m         right_on\u001b[39m=\u001b[39;49mright_on,\n\u001b[1;32m    155\u001b[0m         left_index\u001b[39m=\u001b[39;49mleft_index,\n\u001b[1;32m    156\u001b[0m         right_index\u001b[39m=\u001b[39;49mright_index,\n\u001b[1;32m    157\u001b[0m         sort\u001b[39m=\u001b[39;49msort,\n\u001b[1;32m    158\u001b[0m         suffixes\u001b[39m=\u001b[39;49msuffixes,\n\u001b[1;32m    159\u001b[0m         indicator\u001b[39m=\u001b[39;49mindicator,\n\u001b[1;32m    160\u001b[0m         validate\u001b[39m=\u001b[39;49mvalidate,\n\u001b[1;32m    161\u001b[0m     )\n\u001b[1;32m    162\u001b[0m     \u001b[39mreturn\u001b[39;00m op\u001b[39m.\u001b[39mget_result(copy\u001b[39m=\u001b[39mcopy)\n",
      "File \u001b[0;32m~/Envs/adb/lib/python3.11/site-packages/pandas/core/reshape/merge.py:741\u001b[0m, in \u001b[0;36m_MergeOperation.__init__\u001b[0;34m(self, left, right, how, on, left_on, right_on, axis, left_index, right_index, sort, suffixes, indicator, validate)\u001b[0m\n\u001b[1;32m    733\u001b[0m (\n\u001b[1;32m    734\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mleft_join_keys,\n\u001b[1;32m    735\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mright_join_keys,\n\u001b[1;32m    736\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mjoin_names,\n\u001b[1;32m    737\u001b[0m ) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_merge_keys()\n\u001b[1;32m    739\u001b[0m \u001b[39m# validate the merge keys dtypes. We may need to coerce\u001b[39;00m\n\u001b[1;32m    740\u001b[0m \u001b[39m# to avoid incompatible dtypes\u001b[39;00m\n\u001b[0;32m--> 741\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_maybe_coerce_merge_keys()\n\u001b[1;32m    743\u001b[0m \u001b[39m# If argument passed to validate,\u001b[39;00m\n\u001b[1;32m    744\u001b[0m \u001b[39m# check if columns specified as unique\u001b[39;00m\n\u001b[1;32m    745\u001b[0m \u001b[39m# are in fact unique.\u001b[39;00m\n\u001b[1;32m    746\u001b[0m \u001b[39mif\u001b[39;00m validate \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Envs/adb/lib/python3.11/site-packages/pandas/core/reshape/merge.py:1401\u001b[0m, in \u001b[0;36m_MergeOperation._maybe_coerce_merge_keys\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1395\u001b[0m     \u001b[39m# unless we are merging non-string-like with string-like\u001b[39;00m\n\u001b[1;32m   1396\u001b[0m     \u001b[39melif\u001b[39;00m (\n\u001b[1;32m   1397\u001b[0m         inferred_left \u001b[39min\u001b[39;00m string_types \u001b[39mand\u001b[39;00m inferred_right \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m string_types\n\u001b[1;32m   1398\u001b[0m     ) \u001b[39mor\u001b[39;00m (\n\u001b[1;32m   1399\u001b[0m         inferred_right \u001b[39min\u001b[39;00m string_types \u001b[39mand\u001b[39;00m inferred_left \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m string_types\n\u001b[1;32m   1400\u001b[0m     ):\n\u001b[0;32m-> 1401\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(msg)\n\u001b[1;32m   1403\u001b[0m \u001b[39m# datetimelikes must match exactly\u001b[39;00m\n\u001b[1;32m   1404\u001b[0m \u001b[39melif\u001b[39;00m needs_i8_conversion(lk\u001b[39m.\u001b[39mdtype) \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m needs_i8_conversion(rk\u001b[39m.\u001b[39mdtype):\n",
      "\u001b[0;31mValueError\u001b[0m: You are trying to merge on int64 and object columns. If you wish to proceed you should use pd.concat"
     ]
    }
   ],
   "source": [
    "df_os = pd.merge(df_os, va)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consolidate and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(df_es, df_os)\n",
    "df['GVCP_trade_f'] = df['GVC_trade_f'] / df['Exports']\n",
    "df['GVCP_trade_b'] = df['GVC_trade_b'] / df['Exports']\n",
    "df['GVCP_trade'] = df['GVC_trade'] / df['Exports']\n",
    "df['GVCP_prod'] = df['GVC_prod'] / df['va']\n",
    "\n",
    "df = df[[\n",
    "    't', 's', 'agg', 'i', 'Exports', 'va',\n",
    "    'GVC_trade_f', 'GVC_trade_b', 'GVC_trade', 'GVC_prod',\n",
    "    'GVCP_trade_f', 'GVCP_trade_b', 'GVCP_trade', 'GVCP_prod'\n",
    "]]\n",
    "\n",
    "df.to_parquet(f'../data/{output}', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duckdb.sql(f\"SELECT * FROM read_parquet('../data/{output}')\").df()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
